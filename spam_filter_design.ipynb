{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML for Cyber Security - Lab 1, \n",
    "Vinay V Bhapkar, \n",
    "vvb231@nyu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets as skd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_train = skd.load_files('./lingspam_public/lemm_stop/part1',encoding='latin-1',shuffle=False)\n",
    "ls_test = skd.load_files('./lingspam_public/lemm_stop/part2',encoding='latin-1',shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2603, 51611)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "x_train = count_vect.fit_transform(ls_train.data)\n",
    "#x_train = np.nan_to_num(x_train,copy=False)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = count_vect.transform(ls_test.data)\n",
    "x_test = x_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dictionary = (count_vect.get_feature_names())\n",
    "\n",
    "dictionary = Counter(dictionary)\n",
    "\n",
    "dictionary = dictionary.most_common((x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51611"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train1 = x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "IG_vect = np.zeros((x_train1.shape[1],))\n",
    "for i in range(x_train1.shape[1]):\n",
    "IG_vect[i] = sklearn.metrics.mutual_info_score(x_train1[:,i].reshape(x_train1.shape[0]), ls_train.target, contingency=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('IG_vect_lingspam_trial4',IG_vect)\n",
    "#IG_vect = np.load('IG_vect_lingspam_trial4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51611,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IG_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IG1 = np.argsort(IG_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_10 = (-IG_vect).argsort()[:10]\n",
    "top_100 = (-IG_vect).argsort()[:100]\n",
    "top_1000 = (-IG_vect).argsort()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_N_features(x):\n",
    "    N = len(x)\n",
    "    top_N_features = np.zeros(N).astype(object)\n",
    "    for i in range(N):\n",
    "        top_N_features[i] = dictionary[x[i]]\n",
    "    \n",
    "    return top_N_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_10_features:  [('language', 1) ('free', 1) ('remove', 1) ('linguistic', 1)\n",
      " ('university', 1) ('money', 1) ('our', 1) ('click', 1) ('business', 1)\n",
      " ('market', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Get top 10 features\n",
    "top_10_features = get_top_N_features(top_10)\n",
    "print('top_10_features: ',top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_100_features:  [('language', 1) ('free', 1) ('remove', 1) ('linguistic', 1)\n",
      " ('university', 1) ('money', 1) ('our', 1) ('click', 1) ('business', 1)\n",
      " ('market', 1) ('today', 1) ('internet', 1) ('over', 1) ('check', 1)\n",
      " ('product', 1) ('order', 1) ('sell', 1) ('advertise', 1) ('company', 1)\n",
      " ('million', 1) ('100', 1) ('day', 1) ('want', 1) ('english', 1)\n",
      " ('easy', 1) ('best', 1) ('income', 1) ('linguistics', 1) ('save', 1)\n",
      " ('every', 1) ('receive', 1) ('guarantee', 1) ('thousand', 1)\n",
      " ('service', 1) ('bulk', 1) ('mail', 1) ('com', 1) ('buy', 1) ('cash', 1)\n",
      " ('purchase', 1) ('ll', 1) ('cost', 1) ('win', 1) ('edu', 1) ('start', 1)\n",
      " ('dollar', 1) ('address', 1) ('fax', 1) ('mailing', 1) ('offer', 1)\n",
      " ('yourself', 1) ('list', 1) ('papers', 1) ('month', 1) ('hour', 1)\n",
      " ('earn', 1) ('hundred', 1) ('linguist', 1) ('live', 1) ('here', 1)\n",
      " ('success', 1) ('week', 1) ('20', 1) ('theory', 1) ('pay', 1) ('email', 1)\n",
      " ('credit', 1) ('conference', 1) ('ever', 1) ('customer', 1) ('card', 1)\n",
      " ('send', 1) ('profit', 1) ('abstract', 1) ('yours', 1) ('financial', 1)\n",
      " ('need', 1) ('fun', 1) ('speaker', 1) ('watch', 1) ('home', 1) ('name', 1)\n",
      " ('sale', 1) ('bonus', 1) ('zip', 1) ('discussion', 1) ('toll', 1)\n",
      " ('instruction', 1) ('simply', 1) ('syntax', 1) ('amaze', 1) ('online', 1)\n",
      " ('anywhere', 1) ('investment', 1) ('off', 1) ('site', 1) ('department', 1)\n",
      " ('ad', 1) ('program', 1) ('friend', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Get top 100 features\n",
    "top_100_features = get_top_N_features(top_100)\n",
    "print('top_100_features: ',top_100_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_1000_features:  [('language', 1) ('free', 1) ('remove', 1) ('linguistic', 1)\n",
      " ('university', 1) ('money', 1) ('our', 1) ('click', 1) ('business', 1)\n",
      " ('market', 1) ('today', 1) ('internet', 1) ('over', 1) ('check', 1)\n",
      " ('product', 1) ('order', 1) ('sell', 1) ('advertise', 1) ('company', 1)\n",
      " ('million', 1) ('100', 1) ('day', 1) ('want', 1) ('english', 1)\n",
      " ('easy', 1) ('best', 1) ('income', 1) ('linguistics', 1) ('save', 1)\n",
      " ('every', 1) ('receive', 1) ('guarantee', 1) ('thousand', 1)\n",
      " ('service', 1) ('bulk', 1) ('mail', 1) ('com', 1) ('buy', 1) ('cash', 1)\n",
      " ('purchase', 1) ('ll', 1) ('cost', 1) ('win', 1) ('edu', 1) ('start', 1)\n",
      " ('dollar', 1) ('address', 1) ('fax', 1) ('mailing', 1) ('offer', 1)\n",
      " ('yourself', 1) ('list', 1) ('papers', 1) ('month', 1) ('hour', 1)\n",
      " ('earn', 1) ('hundred', 1) ('linguist', 1) ('live', 1) ('here', 1)\n",
      " ('success', 1) ('week', 1) ('20', 1) ('theory', 1) ('pay', 1) ('email', 1)\n",
      " ('credit', 1) ('conference', 1) ('ever', 1) ('customer', 1) ('card', 1)\n",
      " ('send', 1) ('profit', 1) ('abstract', 1) ('yours', 1) ('financial', 1)\n",
      " ('need', 1) ('fun', 1) ('speaker', 1) ('watch', 1) ('home', 1) ('name', 1)\n",
      " ('sale', 1) ('bonus', 1) ('zip', 1) ('discussion', 1) ('toll', 1)\n",
      " ('instruction', 1) ('simply', 1) ('syntax', 1) ('amaze', 1) ('online', 1)\n",
      " ('anywhere', 1) ('investment', 1) ('off', 1) ('site', 1) ('department', 1)\n",
      " ('ad', 1) ('program', 1) ('friend', 1) ('grammar', 1) ('ship', 1)\n",
      " ('huge', 1) ('line', 1) ('dream', 1) ('science', 1) ('back', 1)\n",
      " ('study', 1) ('structure', 1) ('cd', 1) ('wait', 1) ('de', 1)\n",
      " ('report', 1) ('deadline', 1) ('opportunity', 1) ('us', 1) ('step', 1)\n",
      " ('next', 1) ('again', 1) ('package', 1) ('legal', 1) ('mlm', 1)\n",
      " ('research', 1) ('keep', 1) ('security', 1) ('analysis', 1) ('try', 1)\n",
      " ('workshop', 1) ('fresh', 1) ('once', 1) ('down', 1) ('spend', 1)\n",
      " ('topic', 1) ('xxx', 1) ('marketing', 1) ('price', 1) ('amount', 1)\n",
      " ('net', 1) ('secret', 1) ('adult', 1) ('each', 1) ('profitable', 1)\n",
      " ('issue', 1) ('immediately', 1) ('right', 1) ('add', 1) ('fantastic', 1)\n",
      " ('advertisement', 1) ('never', 1) ('risk', 1) ('delete', 1) ('freedom', 1)\n",
      " ('own', 1) ('debt', 1) ('simple', 1) ('reference', 1) ('overnight', 1)\n",
      " ('excite', 1) ('aol', 1) ('message', 1) ('semantic', 1) ('hello', 1)\n",
      " ('discourse', 1) ('join', 1) ('software', 1) ('great', 1) ('even', 1)\n",
      " ('committee', 1) ('bank', 1) ('reply', 1) ('everything', 1)\n",
      " ('delivery', 1) ('development', 1) ('1998', 1) ('orders', 1) ('ac', 1)\n",
      " ('author', 1) ('exactly', 1) ('theoretical', 1) ('generate', 1)\n",
      " ('spam', 1) ('submission', 1) ('sex', 1) ('sure', 1) ('search', 1)\n",
      " ('per', 1) ('nothing', 1) ('speech', 1) ('decide', 1) ('24', 1)\n",
      " ('bill', 1) ('web', 1) ('enter', 1) ('return', 1) ('resell', 1)\n",
      " ('fill', 1) ('hit', 1) ('unlimit', 1) ('is', 1) ('ve', 1) ('hot', 1)\n",
      " ('life', 1) ('much', 1) ('personal', 1) ('50', 1) ('paper', 1) ('888', 1)\n",
      " ('tax', 1) ('sales', 1) ('partner', 1) ('visa', 1) ('stealth', 1)\n",
      " ('tel', 1) ('acquisition', 1) ('letter', 1) ('800', 1) ('work', 1)\n",
      " ('help', 1) ('campaign', 1) ('tell', 1) ('everyone', 1) ('capital', 1)\n",
      " ('re', 1) ('phonology', 1) ('love', 1) ('most', 1) ('absolutely', 1)\n",
      " ('computational', 1) ('hottest', 1) ('video', 1) ('lexical', 1)\n",
      " ('between', 1) ('remember', 1) ('student', 1) ('little', 1) ('big', 1)\n",
      " ('many', 1) ('effective', 1) ('phone', 1) ('total', 1) ('aspect', 1)\n",
      " ('envelope', 1) ('choose', 1) ('don', 1) ('95', 1) ('cognitive', 1)\n",
      " ('engine', 1) ('put', 1) ('amazing', 1) ('making', 1) ('undeliverable', 1)\n",
      " ('plans', 1) ('context', 1) ('monthly', 1) ('always', 1) ('1995', 1)\n",
      " ('read', 1) ('semantics', 1) ('top', 1) ('word', 1) ('works', 1)\n",
      " ('invest', 1) ('500', 1) ('below', 1) ('please', 1) ('guaranteed', 1)\n",
      " ('lottery', 1) ('window', 1) ('visit', 1) ('worldwide', 1) ('future', 1)\n",
      " ('office', 1) ('car', 1) ('french', 1) ('200', 1) ('rate', 1)\n",
      " ('refund', 1) ('print', 1) ('reports', 1) ('luck', 1) ('really', 1)\n",
      " ('prove', 1) ('federal', 1) ('lose', 1) ('brand', 1) ('relax', 1)\n",
      " ('syntactic', 1) ('fortune', 1) ('already', 1) ('clean', 1) ('john', 1)\n",
      " ('chance', 1) ('mastercard', 1) ('weekly', 1) ('summary', 1)\n",
      " ('institute', 1) ('focus', 1) ('affiliation', 1) ('follow', 1) ('verb', 1)\n",
      " ('recruit', 1) ('book', 1) ('hundreds', 1) ('millions', 1) ('hobby', 1)\n",
      " ('extra', 1) ('move', 1) ('construction', 1) ('morphology', 1) ('four', 1)\n",
      " ('cent', 1) ('seven', 1) ('within', 1) ('duplicate', 1) ('faster', 1)\n",
      " ('retire', 1) ('participate', 1) ('charge', 1) ('german', 1)\n",
      " ('translation', 1) ('level', 1) ('multi', 1) ('let', 1)\n",
      " ('corporations', 1) ('dollars', 1) ('session', 1) ('doubt', 1)\n",
      " ('comply', 1) ('powerful', 1) ('lists', 1) ('super', 1) ('advantage', 1)\n",
      " ('city', 1) ('better', 1) ('april', 1) ('create', 1) ('quick', 1)\n",
      " ('invite', 1) ('enjoy', 1) ('role', 1) ('pragmatic', 1) ('totally', 1)\n",
      " ('show', 1) ('practically', 1) ('pp', 1) ('yahoo', 1) ('plus', 1)\n",
      " ('competition', 1) ('suite', 1) ('1302', 1) ('gamble', 1) ('hesitate', 1)\n",
      " ('sources', 1) ('query', 1) ('miss', 1) ('greatest', 1) ('excess', 1)\n",
      " ('chair', 1) ('perspective', 1) ('true', 1) ('completely', 1)\n",
      " ('sender', 1) ('are', 1) ('lot', 1) ('alter', 1) ('european', 1)\n",
      " ('game', 1) ('owe', 1) ('billion', 1) ('id', 1) ('paste', 1)\n",
      " ('historical', 1) ('make', 1) ('researcher', 1) ('box', 1) ('corpus', 1)\n",
      " ('mailer', 1) ('10', 1) ('bankruptcy', 1) ('after', 1) ('teen', 1)\n",
      " ('rom', 1) ('550', 1) ('grammatical', 1) ('evidence', 1) ('everythe', 1)\n",
      " ('speak', 1) ('member', 1) ('submit', 1) ('august', 1) ('away', 1)\n",
      " ('mailbox', 1) ('general', 1) ('programme', 1) ('legitimate', 1)\n",
      " ('registration', 1) ('presentation', 1) ('sentence', 1) ('sincerely', 1)\n",
      " ('modern', 1) ('until', 1) ('code', 1) ('approach', 1) ('expiration', 1)\n",
      " ('two', 1) ('same', 1) ('lucky', 1) ('trade', 1) ('bottom', 1)\n",
      " ('programs', 1) ('ye', 1) ('those', 1) ('downline', 1) ('tips', 1)\n",
      " ('millionaire', 1) ('family', 1) ('native', 1) ('evaluating', 1)\n",
      " ('literature', 1) ('eliminate', 1) ('forever', 1) ('wrap', 1)\n",
      " ('release', 1) ('truly', 1) ('formal', 1) ('quit', 1) ('filter', 1)\n",
      " ('discover', 1) ('run', 1) ('before', 1) ('nl', 1) ('afford', 1)\n",
      " ('junk', 1) ('aim', 1) ('discuss', 1) ('secure', 1) ('constraint', 1)\n",
      " ('beach', 1) ('pick', 1) ('interaction', 1) ('accurately', 1) ('mci', 1)\n",
      " ('1341', 1) ('mortgage', 1) ('scam', 1) ('intrusion', 1) ('vacation', 1)\n",
      " ('deposit', 1) ('call', 1) ('datum', 1) ('stop', 1) ('allow', 1)\n",
      " ('file', 1) ('unsubscribe', 1) ('http', 1) ('addresses', 1)\n",
      " ('representation', 1) ('industry', 1) ('incredible', 1) ('present', 1)\n",
      " ('signature', 1) ('latest', 1) ('services', 1) ('buyer', 1) ('one', 1)\n",
      " ('pass', 1) ('roll', 1) ('honest', 1) ('girl', 1) ('easiest', 1) ('uk', 1)\n",
      " ('society', 1) ('ticket', 1) ('second', 1) ('else', 1) ('academic', 1)\n",
      " ('law', 1) ('real', 1) ('through', 1) ('imagine', 1) ('relevant', 1)\n",
      " ('easily', 1) ('dialect', 1) ('organize', 1) ('ez', 1) ('worth', 1)\n",
      " ('response', 1) ('250', 1) ('succeed', 1) ('effort', 1) ('movie', 1)\n",
      " ('notification', 1) ('cleanest', 1) ('instant', 1) ('capitalfm', 1)\n",
      " ('earnings', 1) ('overload', 1) ('fabulous', 1) ('shop', 1) ('wish', 1)\n",
      " ('server', 1) ('announcement', 1) ('thing', 1) ('download', 1) ('tip', 1)\n",
      " ('culture', 1) ('description', 1) ('ed', 1) ('testimonial', 1) ('fast', 1)\n",
      " ('modem', 1) ('sit', 1) ('gold', 1) ('instructions', 1) ('theme', 1)\n",
      " ('piece', 1) ('verify', 1) ('anyone', 1) ('someone', 1) ('number', 1)\n",
      " ('under', 1) ('germany', 1) ('enterprise', 1) ('speed', 1) ('variety', 1)\n",
      " ('trial', 1) ('forget', 1) ('protect', 1) ('sociolinguistic', 1)\n",
      " ('phonological', 1) ('relation', 1) ('hours', 1) ('staggering', 1)\n",
      " ('released', 1) ('creditor', 1) ('sexually', 1) ('reg', 1) ('totals', 1)\n",
      " ('numbers', 1) ('astonishment', 1) ('cram', 1) ('publication', 1)\n",
      " ('type', 1) ('lexicon', 1) ('place', 1) ('perfectly', 1) ('subject', 1)\n",
      " ('isp', 1) ('proof', 1) ('argument', 1) ('automatically', 1) ('sexual', 1)\n",
      " ('yes', 1) ('less', 1) ('did', 1) ('leave', 1) ('financially', 1)\n",
      " ('pattern', 1) ('31', 1) ('believe', 1) ('comparative', 1) ('vanish', 1)\n",
      " ('few', 1) ('plan', 1) ('interpretation', 1) ('particular', 1)\n",
      " ('earth', 1) ('marketer', 1) ('laugh', 1) ('natural', 1) ('prompt', 1)\n",
      " ('provider', 1) ('alway', 1) ('magazine', 1) ('protection', 1)\n",
      " ('waste', 1) ('started', 1) ('contribution', 1) ('limited', 1)\n",
      " ('accountant', 1) ('framework', 1) ('believer', 1) ('raleigh', 1)\n",
      " ('entrepreneur', 1) ('anytime', 1) ('webmaster', 1) ('estate', 1)\n",
      " ('unlimited', 1) ('desirous', 1) ('spout', 1) ('grumble', 1)\n",
      " ('merciless', 1) ('fairchild', 1) ('juno', 1) ('spokane', 1) ('break', 1)\n",
      " ('upgrade', 1) ('van', 1) ('90', 1) ('robert', 1) ('rights', 1) ('3d', 1)\n",
      " ('cambridge', 1) ('state', 1) ('additional', 1) ('instruct', 1)\n",
      " ('conceal', 1) ('ordering', 1) ('dori', 1) ('computer', 1) ('300', 1)\n",
      " ('professor', 1) ('thank', 1) ('proceedings', 1) ('france', 1)\n",
      " ('publish', 1) ('introduction', 1) ('least', 1) ('cultural', 1)\n",
      " ('news', 1) ('bel', 1) ('entire', 1) ('spanish', 1) ('chat', 1) ('et', 1)\n",
      " ('noun', 1) ('corporation', 1) ('hardcore', 1) ('access', 1) ('air', 1)\n",
      " ('world', 1) ('clearance', 1) ('professional', 1) ('alone', 1)\n",
      " ('complex', 1) ('mit', 1) ('batch', 1) ('99', 1) ('genie', 1)\n",
      " ('lawful', 1) ('weeks', 1) ('stun', 1) ('esq', 1) ('rockland', 1)\n",
      " ('dupe', 1) ('qualify', 1) ('application', 1) ('degree', 1) ('request', 1)\n",
      " ('color', 1) ('postage', 1) ('cross', 1) ('unsolicit', 1)\n",
      " ('expression', 1) ('phonetic', 1) ('fraction', 1) ('extraordinary', 1)\n",
      " ('boy', 1) ('error', 1) ('retirement', 1) ('ours', 1) ('case', 1)\n",
      " ('quickly', 1) ('forum', 1) ('rich', 1) ('variation', 1) ('isbn', 1)\n",
      " ('text', 1) ('length', 1) ('owner', 1) ('am', 1) ('association', 1)\n",
      " ('almost', 1) ('hand', 1) ('days', 1) ('action', 1) ('small', 1)\n",
      " ('among', 1) ('1994', 1) ('editor', 1) ('article', 1) ('proposal', 1)\n",
      " ('advertiser', 1) ('welcome', 1) ('potential', 1) ('client', 1)\n",
      " ('extractor', 1) ('faith', 1) ('anything', 1) ('compliance', 1)\n",
      " ('reach', 1) ('gift', 1) ('turn', 1) ('faculty', 1) ('removed', 1)\n",
      " ('rip', 1) ('robbery', 1) ('awesome', 1) ('prepared', 1) ('3005', 1)\n",
      " ('refinance', 1) ('prodigy', 1) ('selle', 1) ('thousands', 1)\n",
      " ('privacy', 1) ('retail', 1) ('blvd', 1) ('japanese', 1) ('music', 1)\n",
      " ('sent', 1) ('enclose', 1) ('hold', 1) ('acceptance', 1) ('radio', 1)\n",
      " ('record', 1) ('march', 1) ('amateur', 1) ('toy', 1) ('bet', 1)\n",
      " ('require', 1) ('reap', 1) ('470', 1) ('md', 1) ('payable', 1)\n",
      " ('penny', 1) ('wall', 1) ('revenue', 1) ('chapter', 1) ('chinese', 1)\n",
      " ('david', 1) ('deliver', 1) ('tremendous', 1) ('june', 1) ('increase', 1)\n",
      " ('school', 1) ('happen', 1) ('principle', 1) ('test', 1) ('write', 1)\n",
      " ('journal', 1) ('successful', 1) ('gov', 1) ('dictionary', 1)\n",
      " ('particularly', 1) ('trash', 1) ('argue', 1) ('au', 1) ('graduate', 1)\n",
      " ('400', 1) ('dissertation', 1) ('1992', 1) ('store', 1) ('button', 1)\n",
      " ('ram', 1) ('unique', 1) ('loan', 1) ('notion', 1) ('morphological', 1)\n",
      " ('affordable', 1) ('camera', 1) ('info', 1) ('complete', 1)\n",
      " ('whatsoever', 1) ('savings', 1) ('centre', 1) ('removal', 1) ('ling', 1)\n",
      " ('club', 1) ('soon', 1) ('mclaughlin', 1) ('stock', 1) ('daily', 1)\n",
      " ('goal', 1) ('percentage', 1) ('cyber', 1) ('secrets', 1) ('msn', 1)\n",
      " ('living', 1) ('classified', 1) ('emailer', 1) ('fm', 1) ('filled', 1)\n",
      " ('boyfriend', 1) ('premium', 1) ('celebrity', 1) ('1618', 1)\n",
      " ('parallel', 1) ('uni', 1) ('asset', 1) ('skeptical', 1) ('hi', 1)\n",
      " ('1999', 1) ('1993', 1) ('colleague', 1) ('dial', 1) ('rat', 1) ('can', 1)\n",
      " ('meg', 1) ('content', 1) ('inexpensive', 1) ('style', 1) ('casino', 1)\n",
      " ('bin', 1) ('clause', 1) ('distinction', 1) ('lecture', 1) ('shock', 1)\n",
      " ('30', 1) ('dept', 1) ('berlin', 1) ('why', 1) ('model', 1) ('job', 1)\n",
      " ('rush', 1) ('february', 1) ('600', 1) ('important', 1) ('expensive', 1)\n",
      " ('vium', 1) ('netherland', 1) ('collect', 1) ('lifetime', 1)\n",
      " ('instantly', 1) ('multus', 1) ('logic', 1) ('vowel', 1) ('wealth', 1)\n",
      " ('high', 1) ('700', 1) ('must', 1) ('valuable', 1) ('residual', 1)\n",
      " ('postal', 1) ('paradise', 1) ('woman', 1) ('mark', 1)\n",
      " ('communication', 1) ('assume', 1) ('section', 1) ('process', 1)\n",
      " ('operate', 1) ('too', 1) ('entertainment', 1) ('symposium', 1)\n",
      " ('scholar', 1) ('psycholinguistic', 1) ('blast', 1) ('benefits', 1)\n",
      " ('continue', 1) ('able', 1) ('persistent', 1) ('jackson', 1)\n",
      " ('hotmail', 1) ('realistic', 1) ('exclusive', 1) ('using', 1)\n",
      " ('michael', 1) ('cut', 1) ('der', 1) ('organizer', 1) ('richer', 1)\n",
      " ('goods', 1) ('confidential', 1) ('someday', 1) ('included', 1)\n",
      " ('wilburn', 1) ('seller', 1) ('pile', 1) ('vulgarity', 1) ('stamped', 1)\n",
      " ('inflation', 1) ('addressed', 1) ('porn', 1) ('biz', 1) ('win95', 1)\n",
      " ('project', 1) ('course', 1) ('first', 1) ('cds', 1) ('low', 1) ('nc', 1)\n",
      " ('digital', 1) ('currency', 1) ('extremely', 1) ('recent', 1) ('link', 1)\n",
      " ('method', 1) ('phrase', 1) ('sponsor', 1) ('guide', 1) ('winner', 1)\n",
      " ('human', 1) ('password', 1) ('payment', 1) ('term', 1) ('campus', 1)\n",
      " ('empirical', 1) ('holiday', 1) ('immediate', 1) ('golden', 1)\n",
      " ('convenience', 1) ('cable', 1) ('fl', 1) ('relate', 1) ('kid', 1)\n",
      " ('population', 1) ('page', 1) ('scientific', 1) ('ahead', 1) ('jump', 1)\n",
      " ('pc', 1) ('early', 1) ('accommodation', 1) ('prof', 1) ('hr', 1)\n",
      " ('race', 1) ('dutch', 1) ('cloth', 1) ('size', 1) ('object', 1)\n",
      " ('review', 1) ('proud', 1) ('optional', 1) ('sake', 1) ('desire', 1)\n",
      " ('teacher', 1) ('concept', 1) ('compuserve', 1) ('postscript', 1)\n",
      " ('figure', 1) ('criminal', 1) ('grow', 1) ('virtual', 1) ('happy', 1)\n",
      " ('25', 1) ('apply', 1) ('rather', 1) ('central', 1) ('legally', 1)\n",
      " ('exact', 1) ('post', 1) ('generative', 1) ('volume', 1) ('satisfy', 1)\n",
      " ('promotion', 1) ('discipline', 1) ('trust', 1) ('lanse', 1)\n",
      " ('september', 1) ('moment', 1) ('methodology', 1) ('define', 1) ('on', 1)\n",
      " ('exceedingly', 1) ('dave', 1) ('japan', 1) ('consider', 1) ('consist', 1)\n",
      " ('directory', 1) ('196', 1) ('girlfriend', 1) ('amex', 1) ('illegal', 1)\n",
      " ('header', 1) ('countless', 1) ('fastest', 1) ('embark', 1) ('209', 1)\n",
      " ('implication', 1) ('catchy', 1) ('murkowskus', 1) ('unproductive', 1)\n",
      " ('billboard', 1) ('recession', 1) ('poorer', 1) ('profanity', 1)\n",
      " ('flamer', 1) ('wisely', 1) ('pic', 1) ('instructed', 1) ('mega', 1)\n",
      " ('overflow', 1) ('divorce', 1) ('downpayment', 1) ('newsgroup', 1)\n",
      " ('jame', 1) ('several', 1) ('utility', 1) ('149', 1) ('obligation', 1)\n",
      " ('oxford', 1) ('develop', 1) ('dr', 1) ('univ', 1) ('excellent', 1)\n",
      " ('copy', 1) ('peter', 1) ('up', 1) ('encourage', 1) ('install', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Get top 1000 features\n",
    "top_1000_features = get_top_N_features(top_1000)\n",
    "print('top_1000_features: ',top_1000_features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_feature_mat(x,y):\n",
    "    N = len(x)\n",
    "    feature_mat = np.zeros((y.shape[0],N))\n",
    "    for i in range(N):\n",
    "        feature_mat[:,i] = column(y,x[i])\n",
    "    return feature_mat     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature matrix for N = [10,100,1000] with Term Frequency features\n",
    "\n",
    "x_train_top10 = get_new_feature_mat(top_10,x_train1) \n",
    "x_test_top10 = get_new_feature_mat(top_10,x_test)\n",
    "\n",
    "\n",
    "x_train_top100 = get_new_feature_mat(top_100,x_train1) \n",
    "x_test_top100 = get_new_feature_mat(top_100,x_test)\n",
    "\n",
    "\n",
    "x_train_top1000 = get_new_feature_mat(top_1000,x_train1) \n",
    "x_test_top1000 = get_new_feature_mat(top_1000,x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Covert x_train and x_test to binary feature format\n",
    "x_train_binary = np.minimum(x_train1, 1)\n",
    "x_test_binary = np.minimum(x_test,1)\n",
    "\n",
    "\n",
    "# Feature matrix for N = [10,100,1000] with binary features\n",
    "binary_x_train_top10 = get_new_feature_mat(top_10,x_train_binary) \n",
    "binary_x_test_top10 = get_new_feature_mat(top_10,x_test_binary)\n",
    "\n",
    "\n",
    "binary_x_train_top100 = get_new_feature_mat(top_100,x_train_binary) \n",
    "binary_x_test_top100 = get_new_feature_mat(top_100,x_test_binary)\n",
    "\n",
    "\n",
    "binary_x_train_top1000 = get_new_feature_mat(top_1000,x_train_binary) \n",
    "binary_x_test_top1000 = get_new_feature_mat(top_1000,x_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['ham', 'spam']\n",
    "def using_naive_bayes(traintf,trainbn,testtf,testbi,trainlabels,testlabels,topf):   #x1=Tf train, x2=Binary train, t1 = tf test, t2 = binary test\n",
    "    \n",
    "    print('N = {}'.format(topf))\n",
    "    \n",
    "    \n",
    "    # Binary Features\n",
    "    BNli = sklearn.naive_bayes.BernoulliNB() \n",
    "    BNli.fit(trainbn,trainlabels);\n",
    "    pred_BNli = BNli.predict(testbi) \n",
    "    accBNli=BNli.score(testbi,testlabels);\n",
    "    #cm3 = confusion_matrix(testlabels,pred_BNli)\n",
    "    \n",
    "    print('Bernoulli NB with Binary Features')\n",
    "    print('Accuracy:',accBNli)\n",
    "    print(classification_report(testlabels,pred_BNli,target_names=target_names))\n",
    "    \n",
    "    \n",
    "    # Binary features\n",
    "    aBN = sklearn.naive_bayes.MultinomialNB();\n",
    "    aBN.fit(trainbn,trainlabels);\n",
    "    pred_aBN = aBN.predict(testbi)\n",
    "    accBN=aBN.score(testbi,testlabels);\n",
    "    #cm2 = confusion_matrix(testlabels,pred_aBN)\n",
    "    \n",
    "    print('Multinomial NB with Binary Features')\n",
    "    print('Accuracy:',accBN)\n",
    "    print(classification_report(testlabels,pred_aBN,target_names=target_names))\n",
    "    \n",
    "\n",
    "    # term frequency\n",
    "    aTF = sklearn.naive_bayes.MultinomialNB();\n",
    "    aTF.fit(traintf,trainlabels);\n",
    "    pred_aTF = aTF.predict(testtf)\n",
    "    accTF=aTF.score(testtf,testlabels);\n",
    "    #cm1 = confusion_matrix(testlabels,pred_aTF)\n",
    "    \n",
    "    print('MultinomialNB with Term Frequency Feature')\n",
    "    print('Accuracy:',accTF)\n",
    "    print(classification_report(testlabels,pred_aTF,target_names=target_names))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 10\n",
      "Bernoulli NB with Binary Features\n",
      "Accuracy: 0.948453608247\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      0.98      0.97       242\n",
      "       spam       0.87      0.82      0.84        49\n",
      "\n",
      "avg / total       0.95      0.95      0.95       291\n",
      "\n",
      "Multinomial NB with Binary Features\n",
      "Accuracy: 0.951890034364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      0.98      0.97       242\n",
      "       spam       0.89      0.82      0.85        49\n",
      "\n",
      "avg / total       0.95      0.95      0.95       291\n",
      "\n",
      "MultinomialNB with Term Frequency Feature\n",
      "Accuracy: 0.962199312715\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.97      0.98       242\n",
      "       spam       0.85      0.94      0.89        49\n",
      "\n",
      "avg / total       0.96      0.96      0.96       291\n",
      "\n",
      "N = 100\n",
      "Bernoulli NB with Binary Features\n",
      "Accuracy: 0.931271477663\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.93      0.99      0.96       242\n",
      "       spam       0.94      0.63      0.76        49\n",
      "\n",
      "avg / total       0.93      0.93      0.93       291\n",
      "\n",
      "Multinomial NB with Binary Features\n",
      "Accuracy: 0.979381443299\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       242\n",
      "       spam       0.98      0.90      0.94        49\n",
      "\n",
      "avg / total       0.98      0.98      0.98       291\n",
      "\n",
      "MultinomialNB with Term Frequency Feature\n",
      "Accuracy: 0.986254295533\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99       242\n",
      "       spam       0.96      0.96      0.96        49\n",
      "\n",
      "avg / total       0.99      0.99      0.99       291\n",
      "\n",
      "N = 1000\n",
      "Bernoulli NB with Binary Features\n",
      "Accuracy: 0.93470790378\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.93      1.00      0.96       242\n",
      "       spam       1.00      0.61      0.76        49\n",
      "\n",
      "avg / total       0.94      0.93      0.93       291\n",
      "\n",
      "Multinomial NB with Binary Features\n",
      "Accuracy: 0.989690721649\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99       242\n",
      "       spam       1.00      0.94      0.97        49\n",
      "\n",
      "avg / total       0.99      0.99      0.99       291\n",
      "\n",
      "MultinomialNB with Term Frequency Feature\n",
      "Accuracy: 0.989690721649\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99       242\n",
      "       spam       1.00      0.94      0.97        49\n",
      "\n",
      "avg / total       0.99      0.99      0.99       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using_naive_bayes(x_train_top10,binary_x_train_top10,x_test_top10,binary_x_test_top10,ls_train.target,ls_test.target,10)\n",
    "using_naive_bayes(x_train_top100,binary_x_train_top100,x_test_top100,binary_x_test_top100,ls_train.target,ls_test.target,100)\n",
    "using_naive_bayes(x_train_top1000,binary_x_train_top1000,x_test_top1000,binary_x_test_top1000,ls_train.target,ls_test.target,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def SVM_classifier(train_x,train_y,test_X,test_y,C,gamma,topf):\n",
    "    a = SVC(C=C,gamma=gamma)\n",
    "    a.fit(train_x,train_y)\n",
    "    b = a.predict(test_X)\n",
    "    print('SVM Classifier for N = {}'.format(topf))\n",
    "    print('Hyperparameters:','\\n','C = {}'.format(C),'\\n','gamma = {}'.format(gamma))\n",
    "    print('Accuracy:',np.mean(b==test_y))\n",
    "    print(classification_report(test_y,b,target_names=target_names))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier for N = 1000\n",
      "Hyperparameters: \n",
      " C = 1.5 \n",
      " gamma = 0.0074\n",
      "Accuracy: 0.982817869416\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       242\n",
      "       spam       1.00      0.90      0.95        49\n",
      "\n",
      "avg / total       0.98      0.98      0.98       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_classifier(binary_x_train_top1000,ls_train.target,binary_x_test_top1000,ls_test.target,1.5,0.0074,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier for N = 100\n",
      "Hyperparameters: \n",
      " C = 1.5 \n",
      " gamma = 0.0074\n",
      "Accuracy: 0.955326460481\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.95      1.00      0.97       242\n",
      "       spam       1.00      0.73      0.85        49\n",
      "\n",
      "avg / total       0.96      0.96      0.95       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_classifier(binary_x_train_top100,ls_train.target,binary_x_test_top100,ls_test.target,1.5,0.0074,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier for N = 10\n",
      "Hyperparameters: \n",
      " C = 1.5 \n",
      " gamma = 0.0074\n",
      "Accuracy: 0.93470790378\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.93      1.00      0.96       242\n",
      "       spam       0.97      0.63      0.77        49\n",
      "\n",
      "avg / total       0.94      0.93      0.93       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_classifier(binary_x_train_top10,ls_train.target,binary_x_test_top10,ls_test.target,1.5,0.0074,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for eval folder data and to generate results.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfile1 = skd.load_files('./eval',encoding='latin-1',shuffle=False)\n",
    "testfile2 = (count_vect.transform(testfile1.data))\n",
    "testfile3 = np.minimum(testfile2.toarray(),1)\n",
    "testfile4 = get_new_feature_mat(top_1000,testfile3)\n",
    "mNomBN = sklearn.naive_bayes.MultinomialNB();\n",
    "mNomBN.fit(binary_x_train_top1000,ls_train.target);\n",
    "predictions = mNomBN.predict(testfile4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results.txt', 'a') as f:\n",
    "    for i in predictions:\n",
    "        print(i,end='\\n',file=f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
